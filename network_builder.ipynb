{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code builds a weighted network of technology clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "#import spacy\n",
    "import string\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading files with links and pages characteristics\n",
    "link=pd.read_csv('link.csv')\n",
    "link=link[['from','to']]\n",
    "link=link.drop_duplicates()\n",
    "\n",
    "page = pd.read_csv('enwiki-20191001-page.csv')\n",
    "locations_df=pd.read_csv('locations.csv') #list of geolocationos stored in csv format\n",
    "locations='|'.join(locations_df['location'].tolist())\n",
    "stop_page='companion|user|templates|shared ip addresses from educational institutions|categories|category|use mdy dates|use dmy dates|self-published work|cs1|self-published work|images of|lists of|redirects|redirected|userbox|webarchive|infobox|articles|wikipedia|pages|wikidata|files with no machine-readable|non-free|all free|wikimedia|all-free|webarchive template wayback links|wiki|knight|town|village|culture|history|bishop|school|street|student|restaurant|mayor|festival|death|politician|politics|architecture|musical group|musical band|writer|musician|tourist attraction|commander|king|queen|church|museum|university|gallery|cathedral|neighbourhood|companies|company|country|birth|death|missing|year|award|template:|infobox:|fictional|movie|theatre|tournament|styles|names|novel|people|album|artist|ballet|discography|dance|opera|contest|song|singer|actor|actress|performer|journal|magazine|newsapaper|television series|television episode|television channel|television program|bibliograph|genre|poem|country|municipality|village|historic|event|holiday|person|player|biography|religion|religious|company|military conflict|currency|celebrity'\n",
    "\n",
    "page=page[['title', 'id','namespace']]\n",
    "page['title'] = page['title'].str.replace('_',' ')\n",
    "pages_select=page[(page['namespace']==0) | (page['namespace']==14)] #selecting only main articles and category articles\n",
    "pages_select=pages_select.drop(['namespace'], axis=1)\n",
    "pages_select = pages_select[~pages_select[\"title\"].str.contains(locations, na=False)]\n",
    "pages_select[\"title\"] = pages_select[\"title\"].str.lower()\n",
    "pages_select = pages_select[~pages_select['title'].str.contains(stop_page, na=False)]\n",
    "pages_select=pages_select[~pages_select['title'].str.contains(r'[0-9]', na=False)]\n",
    "pages_select=pages_select.reset_index(drop=True)\n",
    "pages_select=pages_select.drop_duplicates()\n",
    "\n",
    "#Making sure we have only 'useful' links -- the ones that connect articles and subcategories only\n",
    "link=pd.merge(link, pages_select, how='inner', left_on='from', right_on='id')\n",
    "\n",
    "link=link[['to','title']]\n",
    "link=link.rename(columns={'to':'link_to','title':'link_from'})\n",
    "link=link.drop_duplicates()\n",
    "link['link_to']=link['link_to'].str.strip()\n",
    "link['link_to']=link['link_to'].astype(str)\n",
    "link['link_to']=link['link_to'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "link['length']=link['link_to'].str.len()\n",
    "link=link[link['length']>2]\n",
    "link=link.drop(['length'], axis=1)\n",
    "link['link_from']=link['link_from'].str.strip()\n",
    "link['link_from']=link['link_from'].astype(str)\n",
    "link['link_from']=link['link_from'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "link['length']=link['link_from'].str.len()\n",
    "link=link[link['length']>2]\n",
    "link=link.drop(['length'], axis=1)\n",
    "\n",
    "#Uploading file with the complete patent-article matches\n",
    "df=pd.read_csv('complete_matches_full.csv')\n",
    "df=df[['article_title']]\n",
    "df=df.drop_duplicates()\n",
    "\n",
    "#Making sure we are not losing stand-alone articles\n",
    "temp=link[['link_to']]\n",
    "temp=temp.drop_duplicates()\n",
    "temp['link_from']=temp['link_to']\n",
    "link=link.append(temp, ignore_index=True)\n",
    "link=link.drop_duplicates()\n",
    "link=pd.merge(link, df, how='inner', right_on='article_title', left_on='link_from')\n",
    "link=link[['link_from', 'link_to']]\n",
    "link=link.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####### Now we are building 2nd order links (between A and C if A and C are connected through B) and 3rd order links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_all=link.copy()\n",
    "link_all['level']=1 #first-hand connection have level 1\n",
    "link=link[['link_from', 'link_to']]\n",
    "\n",
    "link2=pd.merge(link, link, how='inner', left_on='link_to', right_on='link_from')\n",
    "link2=link2[['link_from_x','link_to_y']]\n",
    "link2=link2[link2['link_from_x']!=link2['link_to_y']]\n",
    "link2.columns=['link_from','link_to']\n",
    "link2=link2.drop_duplicates()\n",
    "link2['level']=2 #connections through one mediator have level 2\n",
    "link_all=link_all.append(link2, ignore_index=True)\n",
    "link_all.sort_values(by=['link_from', 'link_to', 'level'], ascending=True)\n",
    "link_all=link_all.drop_duplicates(subset=['link_from', 'link_to'], keep='first')\n",
    "\n",
    "link3=link2.drop(['level'], axis=1)\n",
    "link3=pd.merge(link3, link, how='inner', left_on='link_to', right_on='link_from')\n",
    "link3=link3[['link_from_x','link_to_y']]\n",
    "link3=link3[link3['link_from_x']!=link3['link_to_y']]\n",
    "link3.columns=['link_from','link_to']\n",
    "link3['level']=3 #connections through two mediators have level 3\n",
    "link3=link3[link3['link_to']!=link3['link_from']]\n",
    "link3=link3.drop_duplicates()\n",
    "link_all=link_all.append(link3, ignore_index=True)\n",
    "link_all.sort_values(by=['link_from', 'link_to', 'level'], ascending=True)\n",
    "link_all=link_all.drop_duplicates(subset=['link_from', 'link_to'], keep='first')\n",
    "\n",
    "link_all.to_csv(r'link_all.csv', index = None) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
