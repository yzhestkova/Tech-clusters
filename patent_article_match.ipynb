{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code matches patents to their most similar Wikipedia article by Cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading file with patent abstracts\n",
    "patents_text=pd.read_csv('brf_sum_text.csv', encoding = \"ISO-8859-1\")\n",
    "#Uploading file with patent titles\n",
    "patents_title=pd.read_csv('pat_ass_title.csv')\n",
    "\n",
    "\n",
    "#Truncating abstracts for computational purposes\n",
    "patents_text['abstract']=patents_text['abstract'].str[0:5000]\n",
    "patents_title['patent'] = patents_title['patent'].apply(pd.to_numeric, errors='coerce')\n",
    "patents_text['patent'] = patents_text['patent'].apply(pd.to_numeric, errors='coerce')\n",
    "patents_title=patents_title.dropna()\n",
    "patents_text=patents_text.dropna()\n",
    "\n",
    "#Merging titles and abstracts together\n",
    "patents_text=pd.merge(patents_title, patents_text, how='inner', left_on='patent', right_on='patent')\n",
    "patents_text=patents_text.rename(columns={'abstract':'text'})\n",
    "# Basic cleaning\n",
    "patents_text['text'] = patents_text['text'].str.lower()\n",
    "patents_text['text'].replace(to_replace=r'[^a-z]+', value=' ', inplace=True, regex=True)\n",
    "patents_text['text']=patents_text['text'].str.findall('\\w{2,}').str.join(' ')\n",
    "patents_text['title']=patents_text['title'].str.findall('\\w{3,}').str.join(' ')\n",
    "patents_text['title'] = patents_text['title'].str.lower()\n",
    "patents_text['title'].replace(to_replace=r'[^a-z]+', value=' ', inplace=True, regex=True)\n",
    "\n",
    "#Combining patent abstract and title into one after repeating the title three times\n",
    "patents_text['patents_text']=patents_text['title']+' '\n",
    "patents_text['patents_text']=patents_text['patents_text'].str.repeat(3)\n",
    "patents_text['patent_text']=patents_text['patents_text']+patents_text['text']\n",
    "patents_text=patents_text.rename(columns={'title':'patent_title'})\n",
    "patents_text=patents_text[['patent','patent_title','patent_text']]\n",
    "patents_text['patent_text']=patents_text['patent_text'].astype(str) #checking right type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload selected Wiki articles (after defining the relevant ones)\n",
    "articles=pd.read_csv('selected_wiki_articles.csv')\n",
    "\n",
    "#Basic cleaning\n",
    "articles['text']=articles['text'].str.lower()\n",
    "articles['text']=articles['text'].astype(str)\n",
    "articles['text']=articles['text'].str.findall('\\w{3,}').str.join(' ')\n",
    "#Truncating article text at the same level as patent text\n",
    "articles['text']=articles['text'].str[0:5000]\n",
    "articles['title']=articles['title'].str.lower()\n",
    "articles['title']=articles['title'].astype(str)\n",
    "articles['title']=articles['title'].str.findall('\\w{3,}').str.join(' ')\n",
    "\n",
    "#Combining article text and title into one after repeating the title three times\n",
    "articles['article_text']=articles['title']+' '\n",
    "articles['article_text']=articles['article_text'].str.repeat(3)\n",
    "articles['article_text']=articles['article_text']+articles['text']\n",
    "articles=articles.rename(columns={'title':'article_title'})\n",
    "articles=articles[['article_title','article_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "######## Both patent and Wikipedia articles are now prepared for the match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now the goal is to find the most similar Wikipedia article for each patent using cosine similarity (conditional on passing a threshold of similarity lower bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####### Pieces of the following code related to fast computation of cosine similarity matrix are taken from van den Blog: https://bergvca.github.io/2017/10/14/super-fast-string-matching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for tokenizing and lemmatizing text\n",
    "\n",
    "tokenizer=RegexpTokenizer(r'\\w+')\n",
    "def remove_stopwords(text):\n",
    "    words=[w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    lem_text=[lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_text['patent_text'].dropna(inplace=True)\n",
    "patents_text['patent_text']=patents_text['patent_text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "patents_text['patent_text']=patents_text['patent_text'].apply(lambda x: remove_stopwords(x))\n",
    "patents_text['patent_text']=patents_text['patent_text'].apply(lambda x: word_lemmatizer(x))\n",
    "patents_text['patent_text']=patents_text['patent_text'].str.join(\" \")\n",
    "patents_text['patent_text'].dropna(inplace=True)\n",
    "\n",
    "articles['article_text'].dropna(inplace=True)\n",
    "articles['article_text']=articles['article_text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "articles['article_text']=articles['article_text'].apply(lambda x: remove_stopwords(x))\n",
    "articles['article_text']=articles['article_text'].apply(lambda x: word_lemmatizer(x))\n",
    "articles['article_text']=articles['article_text'].str.join(\" \")\n",
    "articles['article_text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "#Matrix that computes pair-wise cosine similarity between all text files but keeps only ntop of them by score\n",
    "#If similarity score is lower than lower_bound, a pair is ignored\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe. If letting top parameter to kick in, you can first check what the subsample looks like\n",
    "#That is helpful when trying to understand the optimal lower_bound\n",
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    #if top:\n",
    "     #   nr_matches = top\n",
    "    #else:\n",
    "     #   nr_matches = sparsecols.size\n",
    "    \n",
    "    nr_matches = sparsecols.size\n",
    "\n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'patent_text': left_side,\n",
    "                          'article_text': right_side,\n",
    "                           'similarity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can split dataset by groups if you want to paralllelize things or just to track the progress\n",
    "patents_text['row']=patents_text.index\n",
    "patents_text['row']=patents_text['row']//10000\n",
    "groups=patents_text['row'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,groups):\n",
    "    print(\"Processing group {}\".format(i))\n",
    "    t1 = time.time()\n",
    "    df_patents=patents_text[patents_text['row']==i]\n",
    "    df_patents=df_patents.drop(['row'], axis=1)\n",
    "    all_text=np.concatenate((df_patents['patent_text'],df_articles['article_text']))\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    tf_idf_matrix = vectorizer.fit_transform(all_text)\n",
    "    matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 100, 0.3)\n",
    "    matches_df = get_matches_df(matches, all_text)\n",
    "    matches_df=pd.merge(matches_df, df_articles, how='inner', left_on='article_text', right_on='article_text')\n",
    "    matches_df=pd.merge(matches_df, df_patents, how='inner', left_on='patent_text', right_on='patent_text')\n",
    "    matches_df=matches_df.drop_duplicates()\n",
    "    matches_df=matches_df.sort_values(by=['patent','similarity'], ascending=[True,False])\n",
    "    #Choose top-1 article for each patent by similarity\n",
    "    matches_df=matches_df.drop_duplicates(subset=['patent'], keep='first')\n",
    "    matches_df=matches_df.reset_index(drop=True)\n",
    "    if i==1:\n",
    "        all_matches=matches_df\n",
    "    else:\n",
    "        all_matches=all_matches.append(matches_df, ignore_index=True)\n",
    "    del matches_df\n",
    "    del df_patents\n",
    "    t = time.time()-t1\n",
    "    print(\"SELFTIMED:\", t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
