{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code identifies relevant articles for the match with patents using class information from USPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before applying this algorithm, the relevant sample of observations should be selected based on the procedure in \"Identifying relevant articles (part1, based on infobox and title)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "pd.set_option('mode.chained_assignment', None)\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=pd.read_csv('link.csv') #links files after part1 execusion\n",
    "\n",
    "#Selecting on the destination of the link\n",
    "link_cat_to=link[['to']] \n",
    "link_cat_to=link_cat_to.drop_duplicates()\n",
    "link_cat_to['to'] = link_cat_to['to'].str.replace('_',' ')\n",
    "link_cat_to[\"to\"]=' '+link_cat_to[\"to\"]+' '\n",
    "link_cat_to = link_cat_to[~link_cat_to[\"to\"].str.contains(locations, na=False)] #list of location extracted in part1\n",
    "link_cat_to[\"to\"] = link_cat_to[\"to\"].str.lower()\n",
    "link_cat_to = link_cat_to[~link_cat_to[\"to\"].str.contains(stop, na=False)] #list of stop words is in part1\n",
    "link_cat_to=link_cat_to[~link_cat_to['to'].str.contains(r'[0-9]', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading xlsx file with description of patent classes. This can be downloaded from https://www.patentsview.org/download/\n",
    "descr=pd.read_excel('class_description_short.xlsx', index_col=None, header=None)\n",
    "descr.columns = descr.iloc[0]\n",
    "descr=descr.drop(descr.index[0])\n",
    "\n",
    "descr['class_title']=descr['class_title'].str.split(pat = \", \")\n",
    "descr['class_title']=descr['class_title'].astype(str)\n",
    "descr['class_title'] = descr['class_title'].str.replace(\"  \",' ')\n",
    "descr['class_title'] = descr['class_title'].str.replace(\"'\",' ')\n",
    "descr['class_title'] = descr['class_title'].str.replace(\"[\",'')\n",
    "descr['class_title'] = descr['class_title'].str.replace(\"]\",'')\n",
    "descr['class_title'] = descr['class_title'].str.replace(\"  \",' ')\n",
    "descr['class_title']=descr['class_title'].str.split(pat = \",\")\n",
    "descr['class_title']=descr['class_title'].str.join(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepating links dataframe\n",
    "link_cat_to['to']=link_cat_to['to'].str.split(' ')\n",
    "link_cat_to['list']=link_cat_to['to'].astype(str)\n",
    "link_cat_to['list'] = link_cat_to['list'].str.replace('[',' ')\n",
    "link_cat_to['list'] = link_cat_to['list'].str.replace(']',' ')\n",
    "link_cat_to['list'] = link_cat_to['list'].str.replace(\"'\",'')\n",
    "link_cat_to['list'] = link_cat_to['list'].str.replace(\",\",'')\n",
    "link_cat_to['to']=link_cat_to['list']\n",
    "link_cat_to=link_cat_to[['to']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source 1: Collecting all articles that have words from the description of patent classes in their titles\n",
    "for i in range(1,438):\n",
    "    print(\"Processing class {}\".format(i))\n",
    "    cl=descr.loc[i,'class']\n",
    "    lookup=descr.loc[i, 'class_title']\n",
    "    df=link_cat_to[link_cat_to[\"to\"].str.contains(lookup)]\n",
    "    size=df.size\n",
    "    if size>0:\n",
    "        df['patent_class']=cl\n",
    "        df_cat=df_cat.append(df, ignore_index=True)\n",
    "    del df\n",
    "df_cat.to_csv (r'class_category_1.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source 2: Collecting all articles which titles contain any 2-word tokens from the list of the most frequent 2-word tokens in the titles of patents from every class\n",
    "\n",
    "patent_titles=pd.read_csv('patent_title.csv') #data with all titles of patents\n",
    "df=patent_titles[patent_titles['class']==cl]\n",
    "df=df[['title']]\n",
    "work_list=' '.join(df['title'].tolist())\n",
    "import re\n",
    "\n",
    "work_list=re.sub(r'[^\\w\\s]','',work_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of 'stopwords'\n",
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "stopwords += ['beside', 'besides', 'between', 'beyond', 'both']\n",
    "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "stopwords += ['co', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\n",
    "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'product', 'service', 'unit', 'element']\n",
    "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your', 'having', 'make', 'making', 'process', 'producing', 'produce', 'prepar']\n",
    "stopwords += ['yours', 'yourself', 'yourselves', 'method', 'apparatus','device', 'use', 'methods']\n",
    "stopwords += ['thereof', 'therefor', 'processing', 'prodice', 'manufacturing', 'attachment', 'application', 'component']\n",
    "stopwords += ['compound', 'effect', 'high', 'low', 'inner', 'outer', 'manufacture', 'service', 'effective', 'module']\n",
    "\n",
    "import nltk\n",
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux\n",
    "def removeStopwords(wordlist, stopwords):\n",
    "    return [w for w in wordlist if w not in stopwords]\n",
    "\n",
    "work_list=work_list.split()\n",
    "work_list =removeStopwords(work_list, stopwords)\n",
    "dictionary = wordListToFreqDict(work_list)\n",
    "sorteddict = sortFreqDict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For every patent class:\n",
    "\n",
    "for index in range(0,438):\n",
    "    print(\"Processing class {}\".format(index))\n",
    "    cl=descr.loc[index,'class']\n",
    "    df=patent_titles[patent_titles['class']==cl]\n",
    "    df=df[['title']]\n",
    "    \n",
    "    #Randomly pick 100000 patents from the class (if more than 100000 patents in the classs)\n",
    "    if df.size>100000:\n",
    "        df_sample=df.sample(100000)\n",
    "    else:\n",
    "        df_sample=df\n",
    "    sample_list=' '.join(df_sample['title'].tolist())\n",
    "    sample_list=re.sub(r'[^\\w\\s]','',sample_list)\n",
    "    sample_tokens=sample_list.split()\n",
    "    \n",
    "    #Standardizing words\n",
    "    WNlemma = nltk.WordNetLemmatizer()\n",
    "    sample_tokens=[WNlemma.lemmatize(t) for t in sample_tokens] #lemmatizing\n",
    "    sample_tokens =removeStopwords(sample_tokens, stopwords) #removing stopwords\n",
    "    dictionary_s = wordListToFreqDict(sample_tokens)\n",
    "    sorteddict_s = sortFreqDict(dictionary_s)\n",
    "    \n",
    "    #Choosing the most frequent nouns\n",
    "    sorteddict_s=sorteddict_s[0:200]\n",
    "    freq_df = pd.DataFrame(sorteddict_s, columns =['freq', 'word']) \n",
    "    word_list=freq_df['word'].tolist()\n",
    "    word_list = nltk.pos_tag(word_list)\n",
    "    nouns = [word for word,pos in word_list if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    sample_list=' '.join(sample_tokens)\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sample_list)\n",
    "    grams_list = []\n",
    "    \n",
    "    #Picking up noun+verb combo and adj+noun combo\n",
    "    for i in range(len(doc)):\n",
    "        j = i+1\n",
    "        if j < len(doc):\n",
    "            if (doc[i].pos_ == \"NOUN\" and doc[j].pos_ == \"VERB\") or (doc[i].pos_ == \"VERB\" and doc[j].pos_ == \"NOUN\") or (doc[i].pos_ == \"ADJ\" and doc[j].pos_ == \"NOUN\"):\n",
    "                new=doc[i].text+' ' +doc[j].text\n",
    "                grams_list.append(new)\n",
    "    dictionary_s = wordListToFreqDict(grams_list)\n",
    "    sorteddict_s = sortFreqDict(dictionary_s)\n",
    "    \n",
    "    #Choosing the most frequent combinations and adding them to most frequent nouns\n",
    "    sorteddict_s=sorteddict_s[0:200]\n",
    "    freq_df = pd.DataFrame(sorteddict_s, columns =['freq', 'word']) \n",
    "    grams_list=freq_df['word'].tolist()\n",
    "    full_list=grams_list+nouns\n",
    "    df_tokens=df_tokens.append({'patent_class':cl, 'tokens':full_list}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "####### Final step is to choose Wikipedia articles (from what is left after part 1 cleaning) that have non-zero intersection with at least one work token from the patent-class based dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
